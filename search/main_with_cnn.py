import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from gensim.models import Word2Vec
import jieba

# 1. CNNç‰¹å¾æå–å™¨
class CNNFeatureExtractor(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_filters=100, filter_sizes=[2, 3, 4]):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.num_filters = num_filters
        self.filter_sizes = filter_sizes
        
        # å·ç§¯å±‚ï¼šä¸åŒå¤§å°çš„å·ç§¯æ ¸æ•è·ä¸åŒé•¿åº¦çš„çŸ­è¯­ç‰¹å¾
        self.convs = nn.ModuleList([
            nn.Conv1d(embedding_dim, num_filters, kernel_size=fs, padding=1)
            for fs in filter_sizes
        ])
        
        # å…¨è¿æ¥å±‚
        self.fc = nn.Linear(num_filters * len(filter_sizes), embedding_dim)
        
    def forward(self, x):
        # x: [batch_size, seq_len, embedding_dim]
        x = x.transpose(1, 2)  # [batch_size, embedding_dim, seq_len]
        
        # åº”ç”¨ä¸åŒå¤§å°çš„å·ç§¯æ ¸
        conv_outputs = []
        for conv in self.convs:
            conv_out = F.relu(conv(x))  # [batch_size, num_filters, seq_len]
            # å…¨å±€æœ€å¤§æ± åŒ–
            pooled = F.adaptive_max_pool1d(conv_out, 1)  # [batch_size, num_filters, 1]
            conv_outputs.append(pooled.squeeze(2))  # [batch_size, num_filters]
        
        # åˆå¹¶æ‰€æœ‰å·ç§¯ç‰¹å¾
        combined = torch.cat(conv_outputs, dim=1)  # [batch_size, num_filters * len(filter_sizes)]
        
        # æŠ•å½±å›åŸå§‹ç»´åº¦
        output = self.fc(combined)  # [batch_size, embedding_dim]
        return F.normalize(output, p=2, dim=1)  # L2å½’ä¸€åŒ–

# 2. è¯­ä¹‰æœç´¢å¼•æ“
class CNNSemanticSearch:
    def __init__(self, word2vec_model):
        self.model = word2vec_model
        self.vector_size = word2vec_model.vector_size
        self.cnn_extractor = CNNFeatureExtractor(
            vocab_size=len(word2vec_model.wv),
            embedding_dim=self.vector_size
        )
        
    def text_to_cnn_vector(self, text, max_length=50):
        """ä½¿ç”¨CNNæå–æ–‡æœ¬ç‰¹å¾å‘é‡"""
        words = list(jieba.cut(text))[:max_length]  # é™åˆ¶é•¿åº¦
        
        # åˆ›å»ºè¯å‘é‡åºåˆ—
        word_vectors = []
        for word in words:
            if word in self.model.wv:
                word_vectors.append(self.model.wv[word])
        
        if not word_vectors:
            return np.zeros(self.vector_size)
        
        # å¡«å……æˆ–æˆªæ–­åˆ°å›ºå®šé•¿åº¦
        if len(word_vectors) < max_length:
            padding = [np.zeros(self.vector_size)] * (max_length - len(word_vectors))
            word_vectors.extend(padding)
        else:
            word_vectors = word_vectors[:max_length]
        
        # è½¬æ¢ä¸ºtensor
        text_tensor = torch.FloatTensor(np.array(word_vectors)).unsqueeze(0)  # [1, seq_len, emb_dim]
        
        # é€šè¿‡CNNæå–ç‰¹å¾
        with torch.no_grad():
            cnn_vector = self.cnn_extractor(text_tensor)
        
        return cnn_vector.squeeze(0).numpy()

# 3. åŠ è½½æ•°æ®å¹¶æ„å»ºæœç´¢å¼•æ“
print("æ­£åœ¨æ„å»ºCNNæœç´¢å¼•æ“...")

# åŠ è½½Word2Vecæ¨¡å‹
model = Word2Vec.load("word2vec.model")
print(f"Word2Vecæ¨¡å‹åŠ è½½å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(model.wv)}")

# åŠ è½½æ–‡æ¡£
with open("extracted_text.txt", "r", encoding="utf-8") as f:
    documents = [line.strip() for line in f if line.strip()]

print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

# åˆ›å»ºæœç´¢å¼•æ“å®ä¾‹
search_engine = CNNSemanticSearch(model)

# é¢„è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„CNNå‘é‡
print("æ­£åœ¨è®¡ç®—æ–‡æ¡£CNNå‘é‡...")
doc_vectors = []
for i, doc in enumerate(documents):
    if i % 100 == 0:
        print(f"å·²å¤„ç† {i}/{len(documents)} ä¸ªæ–‡æ¡£")
    vec = search_engine.text_to_cnn_vector(doc)
    doc_vectors.append(vec)

doc_vectors = np.array(doc_vectors)
print("æ–‡æ¡£å‘é‡è®¡ç®—å®Œæˆ!")

# 4. æœç´¢å‡½æ•°
def cnn_search(query, top_k=5):
    """ä½¿ç”¨CNNç‰¹å¾è¿›è¡Œæœç´¢"""
    # æå–æŸ¥è¯¢çš„CNNå‘é‡
    query_vector = search_engine.text_to_cnn_vector(query)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarities = np.dot(doc_vectors, query_vector)
    
    # è·å–æœ€ç›¸ä¼¼çš„æ–‡æ¡£
    top_indices = np.argsort(similarities)[::-1][:top_k]
    
    results = []
    for idx in top_indices:
        if similarities[idx] > 0:
            results.append({
                'score': float(similarities[idx]),
                'text': documents[idx],
                'doc_id': int(idx)
            })
    
    return results

# 5. å¯¹æ¯”æœç´¢å‡½æ•°ï¼ˆä½¿ç”¨åŸå§‹Word2Vecå¹³å‡å‘é‡ï¼‰
def word2vec_search(query, top_k=5):
    """ä½¿ç”¨åŸå§‹Word2Vecå¹³å‡å‘é‡æœç´¢"""
    words = list(jieba.cut(query))
    vectors = []
    for word in words:
        if word in model.wv:
            vectors.append(model.wv[word])
    
    if vectors:
        query_vector = np.mean(vectors, axis=0)
        query_vector = query_vector / np.linalg.norm(query_vector)
    else:
        query_vector = np.zeros(model.vector_size)
    
    # è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„å¹³å‡å‘é‡
    doc_w2v_vectors = []
    for doc in documents:
        words = list(jieba.cut(doc))
        vecs = [model.wv[w] for w in words if w in model.wv]
        if vecs:
            doc_vec = np.mean(vecs, axis=0)
            doc_vec = doc_vec / np.linalg.norm(doc_vec)
        else:
            doc_vec = np.zeros(model.vector_size)
        doc_w2v_vectors.append(doc_vec)
    
    doc_w2v_vectors = np.array(doc_w2v_vectors)
    similarities = np.dot(doc_w2v_vectors, query_vector)
    top_indices = np.argsort(similarities)[::-1][:top_k]
    
    results = []
    for idx in top_indices:
        if similarities[idx] > 0:
            results.append({
                'score': float(similarities[idx]),
                'text': documents[idx],
                'doc_id': int(idx)
            })
    
    return results

# 6. äº¤äº’å¼æœç´¢
print("\n=== CNNè¯­ä¹‰æœç´¢å¼•æ“å°±ç»ª ===")
print("è¾“å…¥ 'quit' é€€å‡ºç¨‹åº")
print("è¾“å…¥ 'compare æœç´¢è¯' å¯¹æ¯”ä¸¤ç§æ–¹æ³•")

while True:
    user_input = input("\nğŸ” è¯·è¾“å…¥æœç´¢è¯: ").strip()
    
    if user_input.lower() == 'quit':
        print("å†è§!")
        break
    
    if not user_input:
        continue
    
    if user_input.startswith('compare '):
        query = user_input[8:]
        print(f"\n=== å¯¹æ¯”æœç´¢: '{query}' ===")
        
        print("\nğŸ”¬ CNNæœç´¢ç»“æœ:")
        cnn_results = cnn_search(query, top_k=3)
        for i, result in enumerate(cnn_results, 1):
            print(f"{i}. [CNNç›¸ä¼¼åº¦: {result['score']:.3f}] {result['text'][:80]}...")
        
        print("\nğŸ“Š Word2Vecå¹³å‡å‘é‡æœç´¢ç»“æœ:")
        w2v_results = word2vec_search(query, top_k=3)
        for i, result in enumerate(w2v_results, 1):
            print(f"{i}. [W2Vç›¸ä¼¼åº¦: {result['score']:.3f}] {result['text'][:80]}...")
    
    else:
        results = cnn_search(user_input)
        
        if results:
            print(f"\næ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:")
            for i, result in enumerate(results, 1):
                print(f"\n{i}. ğŸ“„ [CNNç›¸ä¼¼åº¦: {result['score']:.3f}]")
                print(f"   æ–‡æ¡£ {result['doc_id']}: {result['text'][:100]}...")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")